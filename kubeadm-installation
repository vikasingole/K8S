<======= INSTALLATION OF KUBEADM  =========>

===> What is Kubeadm?
     Kubeadm is a tool to set up a minimum viable Kubernetes cluster without much complex configuration. Also,
     Kubeadm makes the whole process easy by running a series of prechecks to ensure that the server has all 
     the essential components and configs to run Kubernetes.

    It is developed and maintained by the official Kubernetes community. There are other options like minikube,
    kind, etc., that are pretty easy to set up. You can check out my minikube tutorial. Those are good options
    with minimum hardware requirements if you are deploying and testing applications on Kubernetes.

    But if you want to play around with the cluster components or test utilities that are part of cluster administration,
    Kubeadm is the best option. Also, you can create a production-like cluster locally on a workstation for development 
    and testing purposes.

===> Kubeadm Setup Prerequisites
     Minimum two Ubuntu nodes [One master and one worker node]. You can have more worker nodes as per your requirement.
     The master node should have a minimum of 2 vCPU and 2GB RAM.
     For the worker nodes, a minimum of 1vCPU and 2 GB RAM is recommended.
     10.X.X.X/X network range with static IPs for master and worker nodes. We will be using the 192.x.x.x series as the pod network range that
     will be used by the Calico network plugin. Make sure the Node IP range and pod IP range donâ€™t overlap.

Note ---> If you are setting up the cluster in the corporate network behind a proxy, ensure set the proxy variables and
          have access to gcr container registry and docker hub. or talk to your network administrator to whitelist k8s.gcr.io to pull the required images.



STEP-0 ===> first set hostname on all nodes
            hostnamectl set-hostname master
            bash
            hostnamectl set-hostname node1 
            bash
            hostnamectl set-hostname node2
            bash
STEP-1 ===>
           Disable swap on all the Nodes
           For kubeadm to work properly, you need to disable swap on all the nodes using the following command.

           swapoff -a
           sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

           The fstab entry will make sure the swap is off on system reboots.


STEP-2 ===>
           The basic requirement for a Kubernetes cluster is a container runtime. You can have any one of the following container runtimes.
           
           1) CRI-O
           2) Containerd
           3) Docker Engine (using cri-dockerd)
           
           here we are using "containerd" runtime instead of CRI-O and Docker Engine.., so we can get containerd runtime parh from containerd website
           (Containerd--->Docs--->Getting started with containerd--->https://github.com/containerd/containerd/blob/main/docs/getting-started.md)
           
          Step-1 Installing containerd
           
           wget https://github.com/containerd/containerd/releases/download/v1.7.0/containerd-1.7.0-linux-amd64.tar.gz  <--- path of archive file of containerd service
           tar Cxzvf /usr/local containerd-1.7.0-linux-amd64.tar.gz 
           
           If we intend to start containerd via systemd, we should also download 'the containerd.service' unit file from
           https://raw.githubusercontent.com/containerd/containerd/main/containerd.service into /usr/local/lib/systemd/system/containerd.service, and run the following commands: 
           systemctl daemon-reload
           systemctl enable --now containerd
          
           wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service
           mkdir -p /usr/local/lib/systemd/system                   <---- here we are creating /systemd/system  <--- no such folder is there      
           mv containerd.service /usr/local/lib/systemd/system/containerd.service
           systemctl daemon-reload
           systemctl enable --now containerd
  
         Step-2 Installing runc
          wget https://github.com/opencontainers/runc/releases/download/v1.1.4/runc.amd64
          install -m 755 runc.amd64 /usr/local/sbin/runc
          
         Step-3 Installing CNI plugins  <---these are container networking plugins
          wget https://github.com/containernetworking/plugins/releases/download/v1.2.0/cni-plugins-linux-amd64-v1.2.0.tgz   
          mkdir -p /opt/cni/bin
          tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.2.0.tgz
      
Interacting with containerd via CLI         
Install CRICTL ===> 
  
VERSION="v1.26.0" # check latest version in /releases page
wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz
sudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin
rm -f crictl-$VERSION-linux-amd64.tar.gz

cat <<EOF | sudo tee /etc/crictl.yaml
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 2
debug: false
pull-image-on-create: false
EOF   <----type EOF after paste all the commands

=====>Forwarding IPv4 and letting iptables see bridged traffic   (here we have forward our ip)
      Execute the below mentioned instructions:

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF
sudo sysctl --system
sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward
modprobe br_netfilter
sysctl -p /etc/sysctl.conf           

<===now we have to install kubectl, kubelet and kubeadm===>

Installing kubeadm, kubelet and kubectl
You will install these packages on all of your machines:

kubeadm: the command to bootstrap the cluster.

kubelet: the component that runs on all of the machines in your cluster and does things like starting pods and containers.

kubectl: the command line util to talk to your cluster.

process===>

apt-get update && sudo apt-get install -y apt-transport-https curl      <----- this (https) package for intra cluster communication (perticularly from control plane to individual pods) (intra means inside communication and inter means outside communication)

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl      <--------sabhiko hold pe rakhne ke lia

---> From Master-Node..,

kubeadm  <---for checkout either install or not
kubeadm --version

===>Run on Master Node and Follow the instructions<===
    (packages required for making "master")

kubeadm config images pull     <---- here we are pulling images (pulling components API-Server, Controller Manager, Schedular, etcd, pause, proxy, coredns)
kubeadm init                   <---- to initialize kubeadm
kubeadm reset                  <---- to reset kubeadm

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.

we can install any CNI plugin, We will install weavenet 

kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml


---> From Slave-Node..,
  
===>Run on Slave Node<===

on slave 1===>
kubeadm join 172.31.30.136:6443 --token s4wpi9.9cfqta42idw0xv5u \
        --discovery-token-ca-cert-hash sha256:725ee40acb1207bf543e179d2ce22a931c80aec9b49d5fd52bcc240a9ef88150

on slave 2===>
kubeadm join 172.31.30.136:6443 --token s4wpi9.9cfqta42idw0xv5u \
        --discovery-token-ca-cert-hash sha256:725ee40acb1207bf543e179d2ce22a931c80aec9b49d5fd52bcc240a9ef88150

they suggest to run command on control plane after joined on slave nodes..,

===>Test the Setup<===

kubectl get nodes
kubectl get pods -A

-----------------------------------------------------------------------------------------------------

on control plane

kubectl get service <---display cluster ip

===>Run a demo app<===

kubectl run nginx --image=nginx --port=80
kubectl expose pod nginx --port=80 --type=NodePort